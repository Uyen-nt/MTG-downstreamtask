# scripts/05_finetune.py
import os
import shutil
import subprocess
import glob

# === C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ===
DATA_DIR = "data"
PRETRAIN_DIR = "/kaggle/working/MTG-downstreamtask/gram/results/pretrain_real"
FINETUNE_DIR = "/kaggle/working/MTG-downstreamtask/gram/results/finetune"
REAL_SEQS = "/kaggle/working/MTG-downstreamtask/data/result/mimic3/real_mimic3.3digitICD9.seqs"
REAL_LABELS = f"{DATA_DIR}/real_mimic3.labels"
TREE = f"{DATA_DIR}/tree_mimic3"

os.makedirs(FINETUNE_DIR, exist_ok=True)

# === T√åM MODEL PRETRAIN (.npz) ===
pretrain_models = sorted(glob.glob(f"{PRETRAIN_DIR}/*.npz"))
if not pretrain_models:
    raise FileNotFoundError(
        f"Kh√¥ng t√¨m th·∫•y model pretrain (.npz) t·∫°i {PRETRAIN_DIR}\n"
        "H√£y ch·∫°y 04_pretrain.py tr∆∞·ªõc!"
    )


best_model = sorted(pretrain_models)[-1]
finetune_init = f"{FINETUNE_DIR}/pretrain_model.npz"
shutil.copy(best_model, finetune_init)
print(f"‚úÖ Loaded pre-trained weights: {best_model}")
print(f"üì¶ Copied to: {finetune_init}")

# === CH·∫†Y GRAM V·ªöI AESARA ===
cmd = [
    "python", "model/gram.py",
    REAL_SEQS,
    REAL_LABELS,
    TREE,
    FINETUNE_DIR,
    "--embed_file", finetune_init,
    "--n_epochs", "50",
    "--batch_size", "100",
    "--rnn_size", "128",
    "--attention_size", "128",
    "--dropout_rate", "0.5",
    "--L2", "0.001",
    "--verbose"
]

print("\nüöÄ Fine-tuning on real MIMIC-III data...")
print("Command:", " ".join(cmd))

result = subprocess.run(cmd, capture_output=True, text=True)
if result.returncode == 0:
    print("‚úÖ HO√ÄN T·∫§T FINETUNE!")
    print(f"‚Üí Model saved in: {FINETUNE_DIR}")
else:
    print("‚ùå L·ªñI T·ª™ model/gram.py:")
    print(result.stderr)
    raise RuntimeError(f"Finetune th·∫•t b·∫°i: {result.returncode}")
